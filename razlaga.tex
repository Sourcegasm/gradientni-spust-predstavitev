\documentclass[a4paper, 12pt]{article}
\usepackage[slovene]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{pgfplots}

\setlength{\parindent}{0px}
\setlength{\parskip}{10px}

\begin{document}

	\section*{Linearna regresija}
	\paragraph{}
	Predstavljajmo si, izvajamo fizikalni poskus. Imamo vzmet, na kateri merimo raztezek v odvisnosti od sile, s katero napenjamo vzmet (na vzmet obešamo uteži z znano težo in merimo raztezek).

	\footnote{Podatki pridobljeni iz: \texttt{http://www.clemson.edu/ces/phoenix/labs/124/shm/}}
	\begin{tabular}{l|lllll}
		$F$ {[}N{]}  & 1  & 2  & 3  & 4  & 5  \\ \hline
		$x$ {[}cm{]} & 10 & 20 & 35 & 55 & 80
	\end{tabular}

	\paragraph{}
	Velja Hookov zakon, $F = k x$, torej sta $x$ in $F$ med seboj linearno odvisna. Poskušali bomo najti premico, ki se bo najbolje prilegala danim točkam v ravnini.

	\paragraph{}
	Če imamo 2 točki $T_1(x_1, y_1), T_2(x_2, y_2)$, lahko brez težav najdemo premico, ki gre točno skozi niju. Če pa je točk več, ni nujno, da obstaja premica, ki gre skozi vse točke.
	Iščemo taka $a$ in $b$, da se bo premica kar najbolje prilegala danim točkam. Če gre premica skozi neko točko $T_i(x_i, y_i)$, potem velja:

	$$0 = a x_i + b - y_i$$

	Ker pa naša premica ne poteka direktno skozi vse točke pride do napake, ki jo bomo v točki $T_i$ označili z $\varepsilon_i$.

	$$\varepsilon_i = a x_i + b - y_i$$

	\paragraph{}
	Napaka je lahko pozitivna ali negativna, odvisno ali točka leži pod ali nad premico. Želimo zmanjšati velikost vseh napak, ne glede na to, ali so pozitivne ali negativne. (Lahko bi preprosto sešteli absolutne vrednosti napak, ampak pozneje funkcije ne bi mogli odvajati.) Zato seštejemo kvadrate vseh napak.

	$$\varepsilon = \sum_{i=1}^{N} \varepsilon_i^2$$
	$$\varepsilon = \sum_{i=1}^{N} (a x_i - b - y_i)^2$$

	\paragraph{}
	Naš cilj je, da minimiziramo to napako. Napako bomo minimizirali s pomo"cjo gradientnega spusta.

	\section*{Gradientni spust}
	\paragraph{}
	Z gradientnim spustom i"s"cemo minimum ali pa maksimum neke funkcije, to pomeni kje ima funkcija najve"cjo oziroma najmanj"so vrednost. Gradientni spust je zelo primitivna metoda kjer se preprosto premikamo v smer kamor funkcija nara"s"ca oziroma pada.

	\paragraph{}
	To si lahko predstavljamo s pomo"cjo slepega "cloveka, ki "zeli priti na vrh Golovca. Ko bo na"s "clovek, poimenujmo ga Lojze, na neki to"cki hriba, ne bo videl v katero smer mora hoditi (ker je slep), "cutil pa bo kako strm je hrib. Na za"cetku, ko je hrib bolj strm, bo Lojze naredil ve"cji korak, ko pa se bo pribli"zeval vrhu in bo hrib postajal manj strm, pa bo delal manj"se korake, ker se lahko druga"ce zgodi, da bo prestopil vrh in se zna"sel na drugi strani Golovca.

	\paragraph{}
	Z bolj matemati"cnimi besedami: gradient nam pove smer naraščanja funkcije. Preden nadaljujemo z gradientom, moramo definirati kaj nam predstavlaj strmino funkcije (nara"s"canje in padanje). To bomo naredili z odvodi.

	\subsection*{Odvod}
	\paragraph{}
	Odvod neke funkcije $f(x)$ je strmina te funkcije v to"cki $x$. Odvod je pozitiven "ce funkcija nara"sca in negativen "ce funkcija pada. Ve"cji kot je odvod, bolj strma je funkcija v dani to"cki. Odvod funkcije $f(x)$, ki ga ozna"cimo z $f'(x)$, nam torej pove, kakšen je koeficient tangente na graf fukncije $ f(x) $ v točki $x$.


	\begin{figure}[!h]
		\centering
		\caption{Odvod}
		\begin{tikzpicture}
		\begin{axis}[axis lines=left, xmin=-5, xmax=6, ymin=-1, ymax=6]
		\addplot[color=black, smooth]{0.2*x^2};

		\addplot[color=red]{0.8*x - 0.8};
		\addplot[color=red, mark=*] coordinates {(2, 0.8)};

		\addplot[color=blue]{0};
		\addplot[color=blue, mark=*] coordinates {(0, 0)};

		\addplot[color=green]{-0.8*x - 0.8};
		\addplot[color=green, mark=*] coordinates {(-2, 0.8)};
		\legend{,,strmina>0,,strmina=0,,strmina<0}
		\end{axis}
		\end{tikzpicture}
	\end{figure}

	\subsection*{Delni odvod}
	\paragraph{}
	Funkcija lahko vzame ve"c kot en parameter, kar pomeni da je odvisna od 2eh spremenljivk. "Ce bi tako funkcijo "zeleli narisati bi potrebovali 3D graf.

	\begin{figure}[h!]
		\centering
		\caption{3D funkcija}
		\begin{tikzpicture}
			\begin{axis}[
			hide axis,
			colormap/cool,
			]
			\addplot3[
			mesh,
			samples=50,
			domain=-8:8,
			]
			{sin(deg(sqrt(x^2+y^2)))/sqrt(x^2+y^2)};
			\legend{$\frac{\sin r}{r}$}
			\end{axis}
		\end{tikzpicture}
	\end{figure}

	\paragraph{}
	Tako funkcijo moramo odvajati po vsaki spremenljivki posebej, kar pomeni da potrebujemo funkcijo $f(x,y)$ odvajati po $x$ in po $y$. S tem dobimo dva odvoda. Odvod po $x$ nam pove kako funkcija nara"s"ca oziroma pada "ce spreminjamo $x$ parameter, odvod po $y$ pa nam seveda pove kako funkcija nara"s"ca oziroma pada po $y$ parametru. Takemu odvodu re"cemo delni odvod, zapi"semo pa ga kot $\frac{\partial f(x,y)}{\partial x}$ za delni odvod po $x$ in $\frac{\partial f(x,y)}{\partial y}$ za delni odvod po $y$.

	Gradient funkcije f(x) je vektor definiran kot:
	$$\nabla f = \begin{pmatrix}\frac{\partial f}{\partial x_{1}} & \frac{\partial f}{\partial x_{2}} & \frac{\partial f}{\partial x_{3}} & ... & \frac{\partial f}{\partial x_{n}}\end{pmatrix}$$

	V našem primeru linearne regresije je gradientni spust:
	$$\nabla \varepsilon =
	\begin{pmatrix}
	\frac{\partial \varepsilon}{\partial a} &
	\frac{\partial \varepsilon}{\partial b}
	\end{pmatrix}$$

	\paragraph{}
	Ker iščemo minimum funckije se bomo premikali v nasprotno smer gradienta. Spremembo spremenljivke $a$ lahko zapišemo kot:
	$$\Delta a = -(\nabla \varepsilon)_1 \cdot \lambda$$
	kjer $\lambda$ predstavlja velikost permika.

	Naše nove spremenljivke so:
	$$ a = a + \Delta a$$
	$$ b = b + \Delta b$$

	\paragraph{}
	S tem smo se premaknili za en korak bližje minimumu funkcije, to pomeni parametrama $a$ in $b$ pri katerih se bo premica najbolje prilegala na"sim to"ckam. Ta postopek ponovimo še n-krat. Večkrat ko ga ponovimo, bolj natančen bo naš rezultat.

	\paragraph{}
	Natančnost našega rezultata je odvisna tudi od $\lambda$ (velikost premika) in začetnih vrednost spremenljivk $a$ in $b$. Z manjšo velikostjo premika bo rezultat bolj natančen, vendar bomo morali postopek večkrat ponoviti. Ker s postopkom isčemo samo lokalne minimume, nam začetna vrednost spremenljivk določi kateri minimum bomo našli.

	\subsection*{Delni odvodi}
	\paragraph{}
	Funkcija je odvisna od večih spremenljivk, zato jo moramo odvajati za vsako spremenljivko posebej. Ker nam odvod ene spremenljivke pove kako se funkcija obnaša samo po tej spremenljivki, temu rečemu delni odvod.


	$$\frac{\partial \varepsilon}{\partial a} =
	\sum_{i=1}^{N} 2 \frac{\partial (a x_i - b - y_i)}{\partial a} =
	2 \sum_{i=1}^{N} (a x_i - b - y_i)x_i$$

	$$\frac{\partial \varepsilon}{\partial b} =
	\sum_{i=1}^{N} 2 \frac{\partial (a x_i - b - y_i)}{\partial b} =
	2 \sum_{i=1}^{N} (a x_i - b - y_i)(-1) = -2 \sum_{i=1}^{N} (a x_i - b - y_i)$$

	Rešitev najdemo tako, da ugotovimo, pri katerih $a$ in $b$ sta odvoda enaka 0. To lahko rešimo z uporabmo matrik, vendar tega ne zamo, zato se bomo tega lotili s primitvno metodo gradientnega spusta.


	\section*{Linerana regresija elipse}
	\paragraph{}
	Imamo $N$ točk krožnice nekega planeta $T_1(x_1, y_1), T_2(x_2, y_2) \ldots, T_n(x_n, y_n)$. Podobno kot pri prej"snjem primeru, bomo tudi tokrat iskali funkcijo, ki se to"ckam najbolj prilega. Tokrat bomo namesto premice iskali elipso, saj se elipsa seveda bolj prilega kro"znici planeta kot pa premica.


	Elipsa je sto"znica, zato zapišemo splošno enačbo za sto"znice:
	$$Ax^2 + Bxy + Cy^2 + Dx + Ey + F = 0$$

	\paragraph{}
	Tako kot prej se nam bo pojavila napaka, ki jo ozna"cimo z $\varepsilon$.
	$$\varepsilon_i = Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F$$
	Za razliko od premice, je oblika sto"znice odvisna od "sestih parametrov namesto dveh. To so: $A, B, C, D, E$ in $F$. Zato bomo torej spreminjali teh "sest parametrov.

	\paragraph{}
	Podobno kot pri premici najprej definiramo skupno napako kot vsoto kvadratov vseh napak:
	\[\varepsilon = \sum_{i=1}^{N}\varepsilon_i\]
	\[\varepsilon = \sum_{i=1}^{N} (Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)^2\]

	\paragraph{}
	Enako kot pri premici moramo na"so napako delno odvajati po spremenjivkah od katerih je na"sa napaka odvisna, da bomo znali te spremenljivke spreminjati pravilno. To pomeni da potrebujemo izra"cunati delni odvod napake po $A, B, C, D, E$ in $F$. Delni odvodi za ena"cbo sto"znic so:

	$$\frac{\partial \varepsilon}{\partial A} = \sum_{i=1}^{N}2(Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)(x_i^2)$$
	$$\frac{\partial \varepsilon}{\partial B} = \sum_{i=1}^{N}2(Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)(x_iy_i)$$
	$$\frac{\partial \varepsilon}{\partial C} = \sum_{i=1}^{N}2(Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)(y_i^2)$$
	$$\frac{\partial \varepsilon}{\partial D} = \sum_{i=1}^{N}2(Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)(x_i)$$
	$$\frac{\partial \varepsilon}{\partial E} = \sum_{i=1}^{N}2(Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)(y_i)$$
	$$\frac{\partial \varepsilon}{\partial F} = \sum_{i=1}^{N}2(Ax_i^2 + Bx_iy_i + Cy_i^2 + Dx_i + Ey_i + F)$$

	\paragraph{}
	Da najdemo najbolj"se parametre, pri katerih bo funkcija imela najmanj"so napako, bomo ponovno uporabili gradientni spust. Za razliko od gradientnega spusti pri eni premici, bomo tokrat spreminjali "sest parametrov. Na"s gradient je torej:

	$$\nabla \varepsilon = \begin{pmatrix}
	\frac{\partial \varepsilon}{\partial A} &
	\frac{\partial \varepsilon}{\partial B} &
	\frac{\partial \varepsilon}{\partial C} &
	\frac{\partial \varepsilon}{\partial D} &
	\frac{\partial \varepsilon}{\partial E} &
	\frac{\partial \varepsilon}{\partial F}
	\end{pmatrix}$$

	To pomeni da so na"se spremembe parametrov:

	$$\Delta A = -(\nabla \varepsilon)_1 \cdot \lambda$$
	$$\Delta B = -(\nabla \varepsilon)_2 \cdot \lambda$$
	$$\Delta C = -(\nabla \varepsilon)_3 \cdot \lambda$$
	$$\Delta D = -(\nabla \varepsilon)_4 \cdot \lambda$$
	$$\Delta E = -(\nabla \varepsilon)_5 \cdot \lambda$$
	$$\Delta F = -(\nabla \varepsilon)_6 \cdot \lambda$$

	Iz "cesar sledi, da so na"si novi parametri podobno kot pri premici:

	$$A = A + \Delta A$$
	$$B = B + \Delta B$$
	$$C = C + \Delta C$$
	$$D = D + \Delta D$$
	$$E = E + \Delta E$$
	$$F = F + \Delta F$$

	\paragraph{}Pravilnost na"sega rezultata je seveda odvisna od tega koliko ponovitev bomo naredili, kolik"sna bo na"sa $\lambda$ in kak"sne za"cetne vrednosti $A, B, C, D, E$ in $F$ smo si izbrali.

	\section*{Logistična regresija}
	\paragraph{}
	Glavna razlika med logistično in linearno regesijo je, da pri linearni regresiji določamo zvezno spremenljivko ($y$ je odvisen od $x$), medtem ko nam pri logistični regresiji model vrne kakšna je verjetnost, da vhodni podatki sodijo v določeno kategorijo.
	Zato se logistična regresija uporablja za klasifikacijo (npr. prepoznavanje števk idr.).


	\section*{Dodatno}
	\subsection*{Ekliptični koordinatni sistem}
	\paragraph{}
	Eden izmed koordinatnih sistemov za določanje lege nebesnih teles. Lokacijo določimo s tremi podatki. Ker so koordinate definirane glede na ravnino zemljine orbite, je $z$ koordinata za Zemljo vedno enaka 0.
	\begin{description}
		\item[$l$] longtituda, ekliptična dolžina, od $0^\circ$ do $360^\circ$.
		\item[$b$] latituda, ekliptična širina od $-90^\circ$ do $90^\circ$.
		\item[$r$] razdalja
	\end{description}

	\paragraph{}
	V kartezične koordinate podatke pretvorimo po formulah:
	$$x = r \cos b \cos l$$
	$$y = r \cos b \sin l$$
	$$z = r \sin b$$

	\paragraph{}
	Ker tiri vseh planetov ležijo v (skoraj) isti ravnini, bomo $z$ koordianto zanemarili.


\end{document}
